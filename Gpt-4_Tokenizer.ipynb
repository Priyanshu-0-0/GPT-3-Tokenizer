{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt4_tokenizer:\n",
    "    def __init__(self,text,vocab_size):\n",
    "        self.text=text\n",
    "        self.vocab_size=vocab_size\n",
    "        self.pair_dict = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def get_pair_counts(self,text):\n",
    "    \n",
    "        for i in range(len(text) - 1):\n",
    "            pair = (text[i],text[i+1])\n",
    "            if pair in self.pair_dict:\n",
    "                self.pair_dict[pair] += 1\n",
    "            else:\n",
    "                self.pair_dict[pair] = 1\n",
    "\n",
    "        return self.pair_dict \n",
    "    \n",
    "\n",
    "    def pair_switch(token,pair,new_pair):\n",
    "        i=0\n",
    "        np=[]\n",
    "        while i < len(token):\n",
    "            if i<len(token)-1 and token[i] and token[i]==pair[0] and token[i+1]==pair[1]:\n",
    "                np.append(new_pair)\n",
    "                i+=2\n",
    "            else :\n",
    "                np.append(token[i])\n",
    "                i+=1\n",
    "        return np\n",
    "    \n",
    "    \n",
    "    def train(self,text,vocab_size,verbose=False):\n",
    "        text = text.encode(\"utf-8\")\n",
    "        text=list(map(int,text))\n",
    "\n",
    "        number_of_merges=vocab_size-256\n",
    "        bpe_tokens=list(text)\n",
    "        \n",
    "        for i in range(number_of_merges):\n",
    "            pair_dict=self.get_pair_counts(bpe_tokens)\n",
    "            pair = max(pair_dict, key=pair_dict.get)\n",
    "            if verbose==False:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Token {pair} is merged as {i+256}\")\n",
    "            bpe_tokens=self.pair_switch(bpe_tokens,pair,i+256)\n",
    "            self.merges[pair] = i+256\n",
    "            return self.merges,bpe_tokens\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self,ids):\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "        tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "        decoded_text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return decoded_text\n",
    "    \n",
    "    def encoder(self,text):\n",
    "        # Convert the text to bytes\n",
    "        tokens=list(text.encode(\"utf-8\"))\n",
    "        tokens=list(map(int,tokens))\n",
    "        while len(tokens)>=2:\n",
    "            pair_counts=self.get_pair_counts(tokens)\n",
    "            pair=min(pair_counts,key=lambda p:self.merges.get(p,float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens=self.merges(tokens,pair,idx)\n",
    "        return tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Bonjour! 🌟 Welcome to the magical world of language! In this vast universe of words, where the sky is the limit, let's embark on a journey together, exploring the beauty and diversity of different languages. ¡Hola amigos! ¿Cómo están? Today, we're going to delve into the wonders of multilingualism. Imagine being able to speak fluently in various tongues, connecting with people from all walks of life. C'est fantastique! 🎉\n",
    "\n",
    "Let's start with English, the language of Shakespeare and modern communication. English is a global language, spoken by millions around the world. From the bustling streets of New York City to the serene countryside of England, English bridges cultures and societies. It's a language of opportunity, innovation, and creativity. So, grab your cup of tea ☕️ and let's dive into the world of English literature and culture.\n",
    "\n",
    "Pero no podemos olvidar el hermoso idioma español. Con sus ricos sonidos y expresiones poéticas, el español nos lleva en un viaje a través de la pasión y el romance. Desde las vibrantes fiestas de España hasta las playas doradas de América Latina, el español es una celebración de la vida y la diversidad. ¡Viva la lengua española! 🇪🇸\n",
    "\n",
    "Maintenant, parlons français! Ah, la langue de l'amour et de la sophistication. Le français est un mélange envoûtant de finesse et d'élégance. De Paris, la ville lumière, aux champs de lavande de la Provence, le français évoque un sentiment de joie de vivre. C'est magnifique! 🥖\n",
    "\n",
    "As we journey through these languages, let's not forget the power of emojis. 😊 Emojis add color and emotion to our digital conversations. Whether it's a smiley face 😄 to brighten someone's day or a heart ❤️ to express love and affection, emojis transcend language barriers and connect us on a deeper level.\n",
    "\n",
    "Now, let's wrap up our linguistic adventure with a toast 🥂 to the beauty of language! May we continue to explore, learn, and appreciate the rich tapestry of words that make our world a more vibrant and interconnected place. Cheers to language! Salud! 🎊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gpt4_tokenizer(text=text, vocab_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merges, bpe_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtrain(text\u001b[38;5;241m=\u001b[39mtext, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "merges, bpe_tokens = tokenizer.train(text=text, vocab_size=500, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
