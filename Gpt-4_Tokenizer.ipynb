{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt4_tokenizer:\n",
    "    def __init__(self,text,vocab_size):\n",
    "        self.text=text\n",
    "        self.vocab_size=vocab_size\n",
    "        self.pair_dict = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def get_pair_counts(self,text):\n",
    "    \n",
    "        for i in range(len(text) - 1):\n",
    "            pair = (text[i],text[i+1])\n",
    "            if pair in self.pair_dict:\n",
    "                self.pair_dict[pair] += 1\n",
    "            else:\n",
    "                self.pair_dict[pair] = 1\n",
    "\n",
    "        return self.pair_dict \n",
    "    \n",
    "\n",
    "    def pair_switch(token,pair,new_pair):\n",
    "        i=0\n",
    "        np=[]\n",
    "        while i < len(token):\n",
    "            if i<len(token)-1 and token[i] and token[i]==pair[0] and token[i+1]==pair[1]:\n",
    "                np.append(new_pair)\n",
    "                i+=2\n",
    "            else :\n",
    "                np.append(token[i])\n",
    "                i+=1\n",
    "        return np\n",
    "    \n",
    "    \n",
    "    def train(self,text,vocab_size,verbose=False):\n",
    "        text = text.encode(\"utf-8\")\n",
    "        text=list(map(int,text))\n",
    "\n",
    "        number_of_merges=vocab_size-256\n",
    "        bpe_tokens=list(text)\n",
    "        \n",
    "        for i in range(number_of_merges):\n",
    "            pair_dict=self.get_pair_counts(bpe_tokens)\n",
    "            pair = max(pair_dict, key=pair_dict.get)\n",
    "            if verbose==False:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Token {pair} is merged as {i+256}\")\n",
    "            bpe_tokens=self.pair_switch(bpe_tokens,pair,i+256)\n",
    "            self.merges[pair] = i+256\n",
    "            return self.merges,bpe_tokens\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self,ids):\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "        tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "        decoded_text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return decoded_text\n",
    "    \n",
    "    def encoder(self,text):\n",
    "        # Convert the text to bytes\n",
    "        tokens=list(text.encode(\"utf-8\"))\n",
    "        tokens=list(map(int,tokens))\n",
    "        while len(tokens)>=2:\n",
    "            pair_counts=self.get_pair_counts(tokens)\n",
    "            pair=min(pair_counts,key=lambda p:self.merges.get(p,float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens=self.merges(tokens,pair,idx)\n",
    "        return tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Bonjour! ðŸŒŸ Welcome to the magical world of language! In this vast universe of words, where the sky is the limit, let's embark on a journey together, exploring the beauty and diversity of different languages. Â¡Hola amigos! Â¿CÃ³mo estÃ¡n? Today, we're going to delve into the wonders of multilingualism. Imagine being able to speak fluently in various tongues, connecting with people from all walks of life. C'est fantastique! ðŸŽ‰\n",
    "\n",
    "Let's start with English, the language of Shakespeare and modern communication. English is a global language, spoken by millions around the world. From the bustling streets of New York City to the serene countryside of England, English bridges cultures and societies. It's a language of opportunity, innovation, and creativity. So, grab your cup of tea â˜•ï¸ and let's dive into the world of English literature and culture.\n",
    "\n",
    "Pero no podemos olvidar el hermoso idioma espaÃ±ol. Con sus ricos sonidos y expresiones poÃ©ticas, el espaÃ±ol nos lleva en un viaje a travÃ©s de la pasiÃ³n y el romance. Desde las vibrantes fiestas de EspaÃ±a hasta las playas doradas de AmÃ©rica Latina, el espaÃ±ol es una celebraciÃ³n de la vida y la diversidad. Â¡Viva la lengua espaÃ±ola! ðŸ‡ªðŸ‡¸\n",
    "\n",
    "Maintenant, parlons franÃ§ais! Ah, la langue de l'amour et de la sophistication. Le franÃ§ais est un mÃ©lange envoÃ»tant de finesse et d'Ã©lÃ©gance. De Paris, la ville lumiÃ¨re, aux champs de lavande de la Provence, le franÃ§ais Ã©voque un sentiment de joie de vivre. C'est magnifique! ðŸ¥–\n",
    "\n",
    "As we journey through these languages, let's not forget the power of emojis. ðŸ˜Š Emojis add color and emotion to our digital conversations. Whether it's a smiley face ðŸ˜„ to brighten someone's day or a heart â¤ï¸ to express love and affection, emojis transcend language barriers and connect us on a deeper level.\n",
    "\n",
    "Now, let's wrap up our linguistic adventure with a toast ðŸ¥‚ to the beauty of language! May we continue to explore, learn, and appreciate the rich tapestry of words that make our world a more vibrant and interconnected place. Cheers to language! Salud! ðŸŽŠ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gpt4_tokenizer(text=text, vocab_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merges, bpe_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtrain(text\u001b[38;5;241m=\u001b[39mtext, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "merges, bpe_tokens = tokenizer.train(text=text, vocab_size=500, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
