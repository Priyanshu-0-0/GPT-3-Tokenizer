{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt4_tokenizer:\n",
    "    def __init__(self,text,vocab_size):\n",
    "        self.text=text\n",
    "        self.vocab_size=vocab_size\n",
    "        self.pair_dict = {}\n",
    "        self.merges = {}\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "\n",
    "\n",
    "    def get_pair_counts(self,text):\n",
    "    \n",
    "        for i in range(len(text) - 1):\n",
    "            pair = (text[i],text[i+1])\n",
    "            if pair in self.pair_dict:\n",
    "                self.pair_dict[pair] += 1\n",
    "            else:\n",
    "                self.pair_dict[pair] = 1\n",
    "\n",
    "        return self.pair_dict \n",
    "    \n",
    "\n",
    "    def pair_switch(self,token,pair,new_pair):\n",
    "        i=0\n",
    "        np=[]\n",
    "        while i < len(token):\n",
    "            if i<len(token)-1 and token[i] and token[i]==pair[0] and token[i+1]==pair[1]:\n",
    "                np.append(new_pair)\n",
    "                i+=2\n",
    "            else :\n",
    "                np.append(token[i])\n",
    "                i+=1\n",
    "        return np\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def train(self, text,vocab_size, verbose=False):\n",
    "        text = text.encode(\"utf-8\")\n",
    "        text=list(map(int,text))\n",
    "\n",
    "        number_of_merges=vocab_size-256\n",
    "        bpe_tokens=list(text)\n",
    "\n",
    "        for i in range(number_of_merges):\n",
    "            pair_dict = self.get_pair_counts(bpe_tokens)\n",
    "            pair = max(pair_dict, key=pair_dict.get)\n",
    "            if verbose:\n",
    "                print(f\"Token {pair} is merged as {i+256}\")\n",
    "            bpe_tokens = self.pair_switch(bpe_tokens, pair, i+256)\n",
    "            self.merges[pair] = i+256\n",
    "        return self.merges, bpe_tokens\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self,ids):\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "        tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "        decoded_text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return decoded_text\n",
    "    \n",
    "    def encoder(self,text):\n",
    "        # Convert the text to bytes\n",
    "        tokens=list(text.encode(\"utf-8\"))\n",
    "        tokens=list(map(int,tokens))\n",
    "        while len(tokens)>=2:\n",
    "            pair_counts=self.get_pair_counts(tokens)\n",
    "            pair=min(pair_counts,key=lambda p:self.merges.get(p,float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens=self.merges(tokens,pair,idx)\n",
    "        return tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEncoder:\n",
    "    def __init__(self, vocab_size=276):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "\n",
    "    def get_pair_counts(self, lst):\n",
    "        pair_dict = {}\n",
    "        for i in range(len(lst) - 1):\n",
    "            pair = (lst[i], lst[i+1])\n",
    "            if pair in pair_dict:\n",
    "                pair_dict[pair] += 1\n",
    "            else:\n",
    "                pair_dict[pair] = 1\n",
    "        return pair_dict\n",
    "\n",
    "    def pair_switch(self, token, pair, new_pair):\n",
    "        i = 0\n",
    "        np = []\n",
    "        while i < len(token):\n",
    "            if i < len(token) - 1 and token[i] and token[i] == pair[0] and token[i+1] == pair[1]:\n",
    "                np.append(new_pair)\n",
    "                i += 2\n",
    "            else:\n",
    "                np.append(token[i])\n",
    "                i += 1\n",
    "        return np\n",
    "\n",
    "    def fit(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        bpe_tokens = list(tokens)\n",
    "        number_of_merges = self.vocab_size - 256\n",
    "        for i in range(number_of_merges):\n",
    "            pair_dict = self.get_pair_counts(bpe_tokens)\n",
    "            if not pair_dict:\n",
    "                break  # No more pairs to merge\n",
    "            pair = max(pair_dict, key=pair_dict.get)\n",
    "            self.merges[pair] = i + 256\n",
    "            bpe_tokens = self.pair_switch(bpe_tokens, pair, i + 256)\n",
    "\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
    "\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        encoded = []\n",
    "        while len(tokens) >= 2:\n",
    "            pair_counts = self.get_pair_counts(tokens)\n",
    "            pair = min(pair_counts, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.pair_switch(tokens, pair, idx)\n",
    "            encoded.append(idx)\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, ids):\n",
    "        decoded = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = decoded.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Bonjour! ðŸŒŸ Welcome to the magical world of language! In this vast universe of words, where the sky is the limit, let's embark on a journey together, exploring the beauty and diversity of different languages. Â¡Hola amigos! Â¿CÃ³mo estÃ¡n? Today, we're going to delve into the wonders of multilingualism. Imagine being able to speak fluently in various tongues, connecting with people from all walks of life. C'est fantastique! ðŸŽ‰\n",
    "\n",
    "Let's start with English, the language of Shakespeare and modern communication. English is a global language, spoken by millions around the world. From the bustling streets of New York City to the serene countryside of England, English bridges cultures and societies. It's a language of opportunity, innovation, and creativity. So, grab your cup of tea â˜•ï¸ and let's dive into the world of English literature and culture.\n",
    "\n",
    "Pero no podemos olvidar el hermoso idioma espaÃ±ol. Con sus ricos sonidos y expresiones poÃ©ticas, el espaÃ±ol nos lleva en un viaje a travÃ©s de la pasiÃ³n y el romance. Desde las vibrantes fiestas de EspaÃ±a hasta las playas doradas de AmÃ©rica Latina, el espaÃ±ol es una celebraciÃ³n de la vida y la diversidad. Â¡Viva la lengua espaÃ±ola! ðŸ‡ªðŸ‡¸\n",
    "\n",
    "Maintenant, parlons franÃ§ais! Ah, la langue de l'amour et de la sophistication. Le franÃ§ais est un mÃ©lange envoÃ»tant de finesse et d'Ã©lÃ©gance. De Paris, la ville lumiÃ¨re, aux champs de lavande de la Provence, le franÃ§ais Ã©voque un sentiment de joie de vivre. C'est magnifique! ðŸ¥–\n",
    "\n",
    "As we journey through these languages, let's not forget the power of emojis. ðŸ˜Š Emojis add color and emotion to our digital conversations. Whether it's a smiley face ðŸ˜„ to brighten someone's day or a heart â¤ï¸ to express love and affection, emojis transcend language barriers and connect us on a deeper level.\n",
    "\n",
    "Now, let's wrap up our linguistic adventure with a toast ðŸ¥‚ to the beauty of language! May we continue to explore, learn, and appreciate the rich tapestry of words that make our world a more vibrant and interconnected place. Cheers to language! Salud! ðŸŽŠ\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gpt4_tokenizer(text=text, vocab_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token (101, 32) is merged as 256\n",
      "Token (115, 32) is merged as 257\n",
      "Token (97, 110) is merged as 258\n",
      "Token (97, 32) is merged as 259\n",
      "Token (32, 116) is merged as 260\n",
      "Token (32, 108) is merged as 261\n",
      "Token (111, 110) is merged as 262\n",
      "Token (101, 115) is merged as 263\n",
      "Token (44, 32) is merged as 264\n",
      "Token (101, 114) is merged as 265\n",
      "Token (101, 114) is merged as 266\n",
      "Token (105, 110) is merged as 267\n",
      "Token (116, 105) is merged as 268\n",
      "Token (100, 32) is merged as 269\n",
      "Token (46, 32) is merged as 270\n",
      "Token (116, 32) is merged as 271\n",
      "Token (111, 114) is merged as 272\n",
      "Token (111, 114) is merged as 273\n",
      "Token (111, 102) is merged as 274\n",
      "Token (103, 117) is merged as 275\n",
      "Token (101, 110) is merged as 276\n",
      "Token (101, 110) is merged as 277\n",
      "Token (100, 256) is merged as 278\n",
      "Token (121, 32) is merged as 279\n",
      "Token (104, 256) is merged as 280\n",
      "Token (111, 117) is merged as 281\n",
      "Token (97, 103) is merged as 282\n",
      "Token (97, 103) is merged as 283\n",
      "Token (117, 114) is merged as 284\n",
      "Token (111, 32) is merged as 285\n",
      "Token (105, 116) is merged as 286\n",
      "Token (97, 114) is merged as 287\n",
      "Token (97, 114) is merged as 288\n",
      "Token (105, 115) is merged as 289\n",
      "Token (101, 116) is merged as 290\n",
      "Token (33, 32) is merged as 291\n",
      "Token (33, 32) is merged as 292\n",
      "Token (33, 32) is merged as 293\n",
      "Token (33, 32) is merged as 294\n",
      "Token (108, 32) is merged as 295\n",
      "Token (240, 159) is merged as 296\n",
      "Token (240, 159) is merged as 297\n",
      "Token (109, 111) is merged as 298\n",
      "Token (109, 111) is merged as 299\n",
      "Token (110, 32) is merged as 300\n",
      "Token (32, 101) is merged as 301\n",
      "Token (110, 103) is merged as 302\n",
      "Token (110, 103) is merged as 303\n",
      "Token (117, 110) is merged as 304\n",
      "Token (39, 257) is merged as 305\n",
      "Token (118, 105) is merged as 306\n",
      "Token (114, 105) is merged as 307\n",
      "Token (108, 97) is merged as 308\n",
      "Token (258, 269) is merged as 309\n",
      "Token (258, 269) is merged as 310\n",
      "Token (258, 269) is merged as 311\n",
      "Token (114, 101) is merged as 312\n",
      "Token (111, 109) is merged as 313\n",
      "Token (195, 169) is merged as 314\n",
      "Token (195, 169) is merged as 315\n",
      "Token (195, 169) is merged as 316\n",
      "Token (105, 257) is merged as 317\n",
      "Token (258, 275) is merged as 318\n",
      "Token (274, 32) is merged as 319\n",
      "Token (274, 32) is merged as 320\n",
      "Token (116, 111) is merged as 321\n",
      "Token (112, 97) is merged as 322\n",
      "Token (112, 97) is merged as 323\n",
      "Token (44, 261) is merged as 324\n",
      "Token (260, 280) is merged as 325\n",
      "Token (260, 280) is merged as 326\n",
      "Token (260, 280) is merged as 327\n",
      "Token (101, 108) is merged as 328\n",
      "Token (101, 108) is merged as 329\n",
      "Token (97, 108) is merged as 330\n",
      "Token (97, 115) is merged as 331\n",
      "Token (100, 105) is merged as 332\n",
      "Token (10, 10) is merged as 333\n",
      "Token (10, 10) is merged as 334\n",
      "Token (10, 10) is merged as 335\n",
      "Token (10, 10) is merged as 336\n",
      "Token (114, 258) is merged as 337\n",
      "Token (108, 259) is merged as 338\n",
      "Token (108, 259) is merged as 339\n",
      "Token (108, 259) is merged as 340\n",
      "Token (108, 259) is merged as 341\n",
      "Token (281, 114) is merged as 342\n",
      "Token (281, 114) is merged as 343\n",
      "Token (281, 114) is merged as 344\n",
      "Token (281, 114) is merged as 345\n",
      "Token (111, 108) is merged as 346\n",
      "Token (111, 108) is merged as 347\n",
      "Token (110, 101) is merged as 348\n",
      "Token (110, 101) is merged as 349\n",
      "Token (268, 262) is merged as 350\n",
      "Token (104, 32) is merged as 351\n",
      "Token (104, 32) is merged as 352\n",
      "Token (104, 32) is merged as 353\n",
      "Token (104, 32) is merged as 354\n",
      "Token (195, 177) is merged as 355\n",
      "Token (119, 272) is merged as 356\n",
      "Token (119, 272) is merged as 357\n",
      "Token (99, 101) is merged as 358\n",
      "Token (101, 257) is merged as 359\n",
      "Token (115, 116) is merged as 360\n",
      "Token (115, 116) is merged as 361\n",
      "Token (115, 116) is merged as 362\n",
      "Token (105, 118) is merged as 363\n",
      "Token (260, 104) is merged as 364\n",
      "Token (99, 262) is merged as 365\n",
      "Token (99, 262) is merged as 366\n",
      "Token (267, 103) is merged as 367\n",
      "Token (267, 103) is merged as 368\n",
      "Token (256, 116) is merged as 369\n",
      "Token (256, 116) is merged as 370\n",
      "Token (256, 116) is merged as 371\n",
      "Token (256, 116) is merged as 372\n",
      "Token (114, 97) is merged as 373\n",
      "Token (114, 97) is merged as 374\n",
      "Token (101, 97) is merged as 375\n",
      "Token (318, 282) is merged as 376\n",
      "Token (100, 97) is merged as 377\n",
      "Token (100, 97) is merged as 378\n",
      "Token (109, 105) is merged as 379\n",
      "Token (109, 105) is merged as 380\n",
      "Token (109, 105) is merged as 381\n",
      "Token (109, 105) is merged as 382\n",
      "Token (112, 111) is merged as 383\n",
      "Token (112, 111) is merged as 384\n",
      "Token (112, 111) is merged as 385\n",
      "Token (112, 111) is merged as 386\n",
      "Token (112, 111) is merged as 387\n",
      "Token (101, 120) is merged as 388\n",
      "Token (101, 120) is merged as 389\n",
      "Token (101, 120) is merged as 390\n",
      "Token (101, 120) is merged as 391\n",
      "Token (101, 120) is merged as 392\n",
      "Token (116, 114) is merged as 393\n",
      "Token (116, 114) is merged as 394\n",
      "Token (116, 114) is merged as 395\n",
      "Token (108, 258) is merged as 396\n",
      "Token (108, 258) is merged as 397\n",
      "Token (112, 108) is merged as 398\n",
      "Token (101, 291) is merged as 399\n",
      "Token (265, 115) is merged as 400\n",
      "Token (104, 265) is merged as 401\n",
      "Token (104, 265) is merged as 402\n",
      "Token (104, 265) is merged as 403\n",
      "Token (264, 101) is merged as 404\n",
      "Token (97, 100) is merged as 405\n",
      "Token (97, 100) is merged as 406\n",
      "Token (270, 67) is merged as 407\n",
      "Token (32, 259) is merged as 408\n",
      "Token (32, 259) is merged as 409\n",
      "Token (260, 111) is merged as 410\n",
      "Token (291, 296) is merged as 411\n",
      "Token (97, 116) is merged as 412\n",
      "Token (69, 302) is merged as 413\n",
      "Token (99, 97) is merged as 414\n",
      "Token (99, 97) is merged as 415\n",
      "Token (104, 97) is merged as 416\n",
      "Token (290, 305) is merged as 417\n",
      "Token (257, 278) is merged as 418\n",
      "Token (108, 289) is merged as 419\n",
      "Token (108, 289) is merged as 420\n",
      "Token (108, 289) is merged as 421\n",
      "Token (108, 289) is merged as 422\n",
      "Token (108, 289) is merged as 423\n",
      "Token (108, 289) is merged as 424\n",
      "Token (108, 289) is merged as 425\n",
      "Token (260, 285) is merged as 426\n",
      "Token (260, 285) is merged as 427\n",
      "Token (260, 285) is merged as 428\n",
      "Token (32, 115) is merged as 429\n",
      "Token (116, 284) is merged as 430\n",
      "Token (108, 108) is merged as 431\n",
      "Token (108, 108) is merged as 432\n",
      "Token (111, 257) is merged as 433\n",
      "Token (111, 257) is merged as 434\n",
      "Token (111, 257) is merged as 435\n",
      "Token (110, 111) is merged as 436\n",
      "Token (108, 105) is merged as 437\n",
      "Token (108, 105) is merged as 438\n",
      "Token (256, 102) is merged as 439\n",
      "Token (256, 102) is merged as 440\n",
      "Token (256, 108) is merged as 441\n",
      "Token (259, 108) is merged as 442\n",
      "Token (259, 108) is merged as 443\n",
      "Token (259, 108) is merged as 444\n",
      "Token (259, 108) is merged as 445\n",
      "Token (107, 32) is merged as 446\n",
      "Token (195, 179) is merged as 447\n",
      "Token (117, 108) is merged as 448\n",
      "Token (113, 117) is merged as 449\n",
      "Token (99, 105) is merged as 450\n",
      "Token (112, 32) is merged as 451\n",
      "Token (195, 167) is merged as 452\n",
      "Token (195, 167) is merged as 453\n",
      "Token (118, 256) is merged as 454\n",
      "Token (118, 256) is merged as 455\n",
      "Token (118, 256) is merged as 456\n",
      "Token (118, 256) is merged as 457\n",
      "Token (118, 256) is merged as 458\n",
      "Token (118, 256) is merged as 459\n",
      "Token (115, 105) is merged as 460\n",
      "Token (115, 105) is merged as 461\n",
      "Token (115, 105) is merged as 462\n",
      "Token (257, 116) is merged as 463\n",
      "Token (265, 257) is merged as 464\n",
      "Token (265, 257) is merged as 465\n",
      "Token (99, 117) is merged as 466\n",
      "Token (99, 117) is merged as 467\n",
      "Token (99, 117) is merged as 468\n",
      "Token (108, 269) is merged as 469\n",
      "Token (108, 269) is merged as 470\n",
      "Token (108, 269) is merged as 471\n",
      "Token (263, 271) is merged as 472\n",
      "Token (263, 322) is merged as 473\n",
      "Token (263, 322) is merged as 474\n",
      "Token (263, 322) is merged as 475\n",
      "Token (274, 261) is merged as 476\n",
      "Token (274, 261) is merged as 477\n",
      "Token (261, 376) is merged as 478\n",
      "Token (261, 376) is merged as 479\n",
      "Token (261, 376) is merged as 480\n",
      "Token (271, 278) is merged as 481\n",
      "Token (271, 278) is merged as 482\n",
      "Token (97, 257) is merged as 483\n",
      "Token (111, 118) is merged as 484\n",
      "Token (111, 118) is merged as 485\n",
      "Token (109, 282) is merged as 486\n",
      "Token (109, 282) is merged as 487\n",
      "Token (109, 282) is merged as 488\n",
      "Token (109, 282) is merged as 489\n",
      "Token (111, 112) is merged as 490\n",
      "Token (119, 286) is merged as 491\n",
      "Token (286, 121) is merged as 492\n",
      "Token (286, 121) is merged as 493\n",
      "Token (101, 287) is merged as 494\n",
      "Token (101, 287) is merged as 495\n",
      "Token (101, 287) is merged as 496\n",
      "Token (263, 116) is merged as 497\n",
      "Token (263, 116) is merged as 498\n",
      "Token (263, 116) is merged as 499\n"
     ]
    }
   ],
   "source": [
    "merges, bpe_tokens = tokenizer.train(text=text,vocab_size=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencoder(text)\n",
      "Cell \u001b[1;32mIn[101], line 75\u001b[0m, in \u001b[0;36mgpt4_tokenizer.encoder\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerges[pair]\n\u001b[1;32m---> 75\u001b[0m     tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "tokenizer.encoder(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "272",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode([\u001b[38;5;241m23\u001b[39m,\u001b[38;5;241m23\u001b[39m])\n",
      "Cell \u001b[1;32mIn[101], line 59\u001b[0m, in \u001b[0;36mgpt4_tokenizer.decode\u001b[1;34m(self, ids)\u001b[0m\n\u001b[0;32m     57\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {idx: \u001b[38;5;28mbytes\u001b[39m([idx]) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m256\u001b[39m)}\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (p0, p1), idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerges\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 59\u001b[0m     vocab[idx] \u001b[38;5;241m=\u001b[39m vocab[p0] \u001b[38;5;241m+\u001b[39m \u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     61\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ids)\n\u001b[0;32m     62\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 272"
     ]
    }
   ],
   "source": [
    "tokenizer.decode([23,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: [256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273]\n",
      "Decoded text: HeHelHellHelloHello,Hello, Hello, hHello, hoHello, howHello, how Hello, how aHello, how arHello, how areHello, how are Hello, how are yHello, how are yoHello, how are youHello, how are you?\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of BPEncoder\n",
    "bpe = BPEncoder(vocab_size=276)\n",
    "\n",
    "# Fit the encoder to a given text\n",
    "text = \"Hello, how are you?\"\n",
    "bpe.fit(text)\n",
    "\n",
    "# Encode text into BPE tokens\n",
    "encoded_text = bpe.encode(text)\n",
    "print(\"Encoded text:\", encoded_text)\n",
    "\n",
    "# Decode BPE tokens back into text\n",
    "decoded_text = bpe.decode(encoded_text)\n",
    "print(\"Decoded text:\", decoded_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
